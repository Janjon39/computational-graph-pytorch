{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6WFLWgjUmfZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4b5985-723c-4697-d57f-697b445c331d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Forward pass (step-by-step) ===\n",
            "\n",
            "Input x = 2.0\n",
            "\n",
            "Layer 1 - Neuron 0: z00 = 1.100000023841858 -> ReLU = 1.100000023841858\n",
            "Layer 1 - Neuron 1: z01 = -2.4000000953674316 -> ReLU = 0.0\n",
            "Layer 1 - Neuron 2: z02 = 1.0999999046325684 -> ReLU = 1.0999999046325684\n",
            "\n",
            "Layer 2 - Neuron 0: z10 = 2.200000047683716 -> Sigmoid = 0.9002495408058167\n",
            "Layer 2 - Neuron 1: z11 = -0.699999988079071 -> Sigmoid = 0.3318122327327728\n",
            "\n",
            "Sum of Layer 1 activations = 2.1999998092651367\n",
            "Sum of Layer 2 activations = 1.232061743736267\n",
            "Combined sum = 3.4320616722106934 -> Tanh = 0.9979130029678345\n",
            "\n",
            "Output layer (linear): output = 0.7981216311454773\n",
            "\n",
            "=== Gradients (after backward) ===\n",
            "d(output)/d(x) = 0.004424192477017641\n",
            "d(output)/d(w00) = 0.007505349349230528\n",
            "d(output)/d(b00) = 0.003752674674615264\n",
            "d(output)/d(w20) = 0.9979130029678345\n",
            "\n",
            "All parameter gradients:\n",
            "  w00: 0.007505\n",
            "  b00: 0.003753\n",
            "  w01: 0.000000\n",
            "  b01: 0.000000\n",
            "  w02: 0.007505\n",
            "  b02: 0.003753\n",
            "  w10: 0.000674\n",
            "  b10: 0.000337\n",
            "  w11: 0.001664\n",
            "  b11: 0.000832\n",
            "  w20: 0.997913\n",
            "  b20: 1.000000\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# MANUAL COMPUTATIONAL GRAPH IN PYTORCH (with gradients)\n",
        "# ======================================================\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.set_printoptions(precision=6, sci_mode=False)\n",
        "\n",
        "# --- Input ---\n",
        "x = torch.tensor(2.0, requires_grad=True)  # scalar input\n",
        "\n",
        "# --- Layer 1 (3 neurons) ---\n",
        "w00 = torch.tensor(0.5, requires_grad=True)\n",
        "b00 = torch.tensor(0.1, requires_grad=True)\n",
        "\n",
        "w01 = torch.tensor(-1.2, requires_grad=True)\n",
        "b01 = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "w02 = torch.tensor(0.7, requires_grad=True)\n",
        "b02 = torch.tensor(-0.3, requires_grad=True)\n",
        "\n",
        "# --- Layer 2 (2 neurons) ---\n",
        "w10 = torch.tensor(1.0, requires_grad=True)\n",
        "b10 = torch.tensor(0.2, requires_grad=True)\n",
        "\n",
        "w11 = torch.tensor(-0.5, requires_grad=True)\n",
        "b11 = torch.tensor(0.3, requires_grad=True)\n",
        "\n",
        "# --- Output layer ---\n",
        "w20 = torch.tensor(0.9, requires_grad=True)\n",
        "b20 = torch.tensor(-0.1, requires_grad=True)\n",
        "\n",
        "# ======================================================\n",
        "# Forward pass (manual computation step by step)\n",
        "# ======================================================\n",
        "\n",
        "print(\"=== Forward pass (step-by-step) ===\\n\")\n",
        "print(f\"Input x = {x.item()}\\n\")\n",
        "\n",
        "# --- Layer 1 (ReLU) ---\n",
        "z00 = w00 * x + b00\n",
        "a00 = torch.relu(z00)\n",
        "print(\"Layer 1 - Neuron 0: z00 =\", z00.item(), \"-> ReLU =\", a00.item())\n",
        "\n",
        "z01 = w01 * x + b01\n",
        "a01 = torch.relu(z01)\n",
        "print(\"Layer 1 - Neuron 1: z01 =\", z01.item(), \"-> ReLU =\", a01.item())\n",
        "\n",
        "z02 = w02 * x + b02\n",
        "a02 = torch.relu(z02)\n",
        "print(\"Layer 1 - Neuron 2: z02 =\", z02.item(), \"-> ReLU =\", a02.item())\n",
        "\n",
        "# --- Layer 2 (Sigmoid) ---\n",
        "z10 = w10 * x + b10\n",
        "a10 = torch.sigmoid(z10)\n",
        "print(\"\\nLayer 2 - Neuron 0: z10 =\", z10.item(), \"-> Sigmoid =\", a10.item())\n",
        "\n",
        "z11 = w11 * x + b11\n",
        "a11 = torch.sigmoid(z11)\n",
        "print(\"Layer 2 - Neuron 1: z11 =\", z11.item(), \"-> Sigmoid =\", a11.item())\n",
        "\n",
        "# --- Combine outputs (+) then Tanh ---\n",
        "sum_layer1 = a00 + a01 + a02\n",
        "sum_layer2 = a10 + a11\n",
        "combined_sum = sum_layer1 + sum_layer2\n",
        "c = torch.tanh(combined_sum)\n",
        "\n",
        "print(\"\\nSum of Layer 1 activations =\", sum_layer1.item())\n",
        "print(\"Sum of Layer 2 activations =\", sum_layer2.item())\n",
        "print(\"Combined sum =\", combined_sum.item(), \"-> Tanh =\", c.item())\n",
        "\n",
        "# --- Output layer (linear) ---\n",
        "output = w20 * c + b20\n",
        "print(\"\\nOutput layer (linear): output =\", output.item())\n",
        "\n",
        "# ======================================================\n",
        "# Backward pass (compute gradients)\n",
        "# ======================================================\n",
        "\n",
        "# Reset gradients if they exist\n",
        "for t in [x, w00, b00, w01, b01, w02, b02, w10, b10, w11, b11, w20, b20]:\n",
        "    if t.grad is not None:\n",
        "        t.grad.zero_()\n",
        "\n",
        "output.backward()  # automatic differentiation\n",
        "\n",
        "print(\"\\n=== Gradients (after backward) ===\")\n",
        "print(\"d(output)/d(x) =\", x.grad.item())\n",
        "print(\"d(output)/d(w00) =\", w00.grad.item())\n",
        "print(\"d(output)/d(b00) =\", b00.grad.item())\n",
        "print(\"d(output)/d(w20) =\", w20.grad.item())\n",
        "\n",
        "# Print all gradients neatly\n",
        "params = {\n",
        "    'w00': w00.grad.item(), 'b00': b00.grad.item(),\n",
        "    'w01': w01.grad.item(), 'b01': b01.grad.item(),\n",
        "    'w02': w02.grad.item(), 'b02': b02.grad.item(),\n",
        "    'w10': w10.grad.item(), 'b10': b10.grad.item(),\n",
        "    'w11': w11.grad.item(), 'b11': b11.grad.item(),\n",
        "    'w20': w20.grad.item(), 'b20': b20.grad.item()\n",
        "}\n",
        "print(\"\\nAll parameter gradients:\")\n",
        "for k, v in params.items():\n",
        "    print(f\" {k:>4}: {v:.6f}\")\n"
      ]
    }
  ]
}